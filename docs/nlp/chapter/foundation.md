# 基本概念

# NLP

## 概念

NLP（自然语言处理，Natural Language Processing）是人工智能领域的重要分支，旨在让计算机能够理解、分析、生成和处理人类语言。NLP融合了语言学、计算机科学和数学，主要任务包括：
- 语言理解：如文本分类、情感分析、命名实体识别、机器翻译等
- 语言生成：如自动摘要、对话系统、文本生成等
- 语音处理：如语音识别、语音合成等

NLP的核心挑战在于语言的复杂性和多样性。近年来，深度学习和预训练语言模型（如PLM、LLM）的发展极大推动了NLP技术进步，使得机器在文本理解和生成方面取得了显著突破。

## PLM（Pre-trained Language Model，预训练语言模型）

**定义**：是一种基于深度学习的模型，通过在大规模无标注文本数据上进行自监督预训练，学习语言的语法、语义和知识表示。

**核心特点**：
- 采用自监督学习范式（通常为掩码语言模型、下一句预测等任务）
- 具有迁移学习能力，可以快速适应各种下游任务
- 包含数百万到数十亿的参数
- 通过微调（Fine-tuning）或少样本学习适配特定应用

**关键优势**：
- 通用性强：单一模型可适用于多个NLP任务
- 学习效率高：利用预训练权重显著减少下游任务所需数据量
- 性能优越：相对于仅用监督学习训练的模型有显著提升

## LLM（Large Language Model，大型语言模型）

**定义**：是参数规模达到十亿级（Billion）以上的预训练语言模型，通常指参数数量在10亿以上的模型。

**主要特性**：
- **规模超大**：参数数量通常为 $10^9 \sim 10^{12}$ 以上
- **涌现能力**：具有传统小规模模型不具备的能力
  - 上下文学习（In-context Learning）
  - 思维链推理（Chain-of-Thought）
  - 指令遵循（Instruction Following）
- **通用性**：可处理文本生成、问题回答、推理、翻译等多种任务

**典型代表**：
- GPT系列（GPT-2、GPT-3、GPT-4）
- Claude系列
- LLaMA系列
- 千问、通义、文心等

## PLM 与 LLM 的区别与联系

| 维度 | PLM | LLM |
|------|------|------|
| **参数规模** | 较小到中等（通常<10B） | 超大（>10B，常见百B+） |
| **训练数据** | 中等规模（数GB-数TB） | 超大规模（数TB-数十TB） |
| **涌现能力** | 无或有限 | 明显涌现能力 |
| **主要应用** | 特定任务微调 | 通用对话、推理、代码生成 |
| **计算成本** | 相对较低 | 极高 |
| **示例** | BERT、RoBERTa | GPT-3、Claude、LLaMA-70B |

**关键联系**：
- LLM 是大规模 PLM 的演化形式
- LLM 基于 PLM 的预训练范式构建
- 二者都采用自监督预训练 + 微调框架

---

# 神经网络基础架构

>[!note]
> 神经网络则是实现 `PLM` 与 `LLM` 框架的底层组件

## FNN（Fully Connected Neural Network，全连接神经网络）

**基本特征**：
- 最简单的人工神经网络结构
- 相邻层的所有神经元两两相连
- 每条连接都有相应的权重参数

**数学表示**：
$$h^{(l)} = \sigma(W^{(l)} \cdot h^{(l-1)} + b^{(l)})$$

其中 $W^{(l)}$ 是权重矩阵，$b^{(l)}$ 是偏置向量，$\sigma$ 是激活函数。

**主要应用**：
- 表格数据分类
- 小规模图像分类（需要扁平化处理）
- 回归问题

**局限性**：
- 参数量巨大（对于高维输入如图像）
- 无法捕捉空间相关性信息
- 计算效率低下

## CNN（Convolutional Neural Network，卷积神经网络）

**核心创新**：
- 引入卷积操作（Convolution），利用局部连接和参数共享
- 通过卷积核在输入上滑动来提取特征
- 层级式特征提取（底层边缘→中层纹理→高层语义）

**关键组件**：
1. **卷积层（Convolutional Layer）**
   - 卷积运算：$y = \rho \cdot I + b$（其中 $\rho$ 为卷积核，$I$ 为输入）
   - 参数共享显著减少模型参数

2. **池化层（Pooling Layer）**
   - 最大池化（Max Pooling）：保留局部最強激活
   - 平均池化（Average Pooling）：计算局部平均值
   - 降维和去噪

3. **激活函数**：ReLU、Leaky ReLU等

**主要优势**：
- 参数效率高：通过卷积核共享和权重绑定大幅减少参数
- 平移不变性：对图像平移具有鲁棒性
- 局部感受野：捕捉局部特征

**典型应用**：
- 图像分类、检测、分割
- 计算机视觉任务
- 医学影像分析

**典型模型**：
- AlexNet、VGGNet、ResNet、DenseNet等

## RNN（Recurrent Neural Network，循环神经网络）

**核心思想**：
- 处理序列数据的神经网络
- 具有时间循环结构，能够建模序列间的依赖关系
- 隐状态在时间步间传递信息

**基本公式**：
$$
\begin{align*}
h_t &= \sigma(W_{hh} \cdot h_{t-1} + W_{xh} \cdot x_t + b) \\
y_t &= W_{hy} \cdot h_t + c
\end{align*}
$$

**主要变体**：

1. **LSTM（Long Short-Term Memory）**
   - 解决梯度消失/爆炸问题
   - 引入门机制（输入门、遗忘门、输出门）
   - 细胞状态传递长期依赖
   ```
   遗忘门：f_t = σ(W_f · [h_{t-1}, x_t] + b_f)
   输入门：i_t = σ(W_i · [h_{t-1}, x_t] + b_i)
   输出门：o_t = σ(W_o · [h_{t-1}, x_t] + b_o)
   细胞状态更新：C_t = f_t ⊙ C_{t-1} + i_t ⊙ tanh(W_C · [h_{t-1}, x_t] + b_C)
   隐状态：h_t = o_t ⊙ tanh(C_t)
   ```

2. **GRU（Gated Recurrent Unit）**
   - LSTM的简化版本
   - 只有重置门和更新门
   - 参数更少，计算更快

**主要应用**：
- 自然语言处理（机器翻译、文本分类、命名实体识别）
- 时间序列预测
- 语音识别
- 机器翻译

**局限性**：
- 长序列上仍可能出现梯度消失
- 并行计算能力有限
- 无法高效处理极长依赖

### 2.4 Transformer（变换器架构）

**基本特征**：
- 基于自注意力机制（Self-Attention）
- 完全抛弃循环和卷积结构
- 并行处理整个序列，计算效率高

**核心组件**：

### 自注意力机制（Self-Attention）

**计算过程**：
1. 线性变换生成查询、键、值向量
   $$Q = X \cdot W^Q, \quad K = X \cdot W^K, \quad V = X \cdot W^V$$

2. 计算注意力权重
   $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

3. 多头注意力（Multi-Head Attention）
   - 并行计算多个注意力表示
   - 捕捉不同角度的特征关系

### 前馈网络（Feed-Forward Network）

$$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$$

具有两层全连接，中间通常为 $4d_{model}$ 维度。

### 编码器-解码器架构

- **编码器**：处理输入序列，生成表示
- **解码器**：逐步生成输出序列，使用掩码自注意力

**主要优势**：
- **并行效率高**：可并行处理序列所有位置
- **长距离依赖**：直接建模任意两个位置间的关系
- **灵活性**：可用于编码-解码、仅编码、仅解码等多种形式

**位置编码（Positional Encoding）**：
$$PE_{(pos,2i)} = \sin(pos / 10000^{2i/d_{model}})$$
$$PE_{(pos,2i+1)} = \cos(pos / 10000^{2i/d_{model}})$$

**典型模型**：
- BERT（双向编码表示）
- GPT（自回归解码）
- T5（编码-解码）
- RoBERTa、ALBERT、XLNet等

---

# 核心概念

## 预训练任务（Pre-training Tasks）

### 掩码语言模型（Masked Language Modeling, MLM）
- **原理**：随机掩码15%的token，预测被掩码的词
- **特点**：产生双向上下文理解
- **代表模型**：BERT

### 下一句预测（Next Sentence Prediction, NSP）/句子排序（Sentence Order Prediction, SOP）
- **原理**：判断两个句子是否相邻或其顺序
- **作用**：学习句子间的关系
- **改进**：SOP比NSP更有效

### 因果语言建模（Causal Language Modeling, CLM）
- **原理**：根据前文预测下一个token
- **特点**：自回归方式，产生单向上下文
- **代表模型**：GPT

### 置换语言建模（Permutation Language Modeling, PLM）
- **原理**：随机排列序列位置，预测被遮挡词
- **特点**：双向性与自回归的结合
- **代表模型**：XLNet

## 微调方法（Fine-tuning Methods）

### 全参数微调（Full Fine-tuning）
- 更新模型所有参数
- 需要大量内存和计算资源
- 精度最高

### 适配器微调（Adapter）
- 在各层间插入小型适配器模块
- 仅训练适配器参数
- 参数高效，存储友好

### LoRA（Low-Rank Adaptation）
- 使用低秩矩阵近似权重更新
- 参数效率极高（通常<1%）
- 推理零开销

### QLoRA（Quantized LoRA）
- LoRA + 量化
- 极大节省内存
- 实现消费级GPU上的大模型微调

## 上下文学习（In-Context Learning, ICL）

**定义**：模型在输入中提供任务示例和描述，不更新参数即可适应新任务。

**学习方式**：
- **Zero-shot**：无示例，仅输入任务描述
- **Few-shot**：提供少量示例（通常2-5个）
- **Chain-of-Thought（CoT）**：提供推理过程示例

**优势**：
- 无需梯度更新
- 快速适应新任务
- 减少对标注数据的依赖

## 思维链（Chain-of-Thought, CoT）

**原理**：促使模型在给出最终答案前生成中间推理步骤。

**效果**：
- 改善复杂推理任务性能
- 提高可解释性
- 对于数学、逻辑推理任务特别有效

**示例提示词**：
```
问题：5个苹果加3个橙子是多少个水果？
思路：将苹果数和橙子数相加：5 + 3 = 8
答案：8个水果
```

## 指令微调（Instruction Fine-tuning）

**定义**：使用"指令-回应"数据对微调模型，使其更好地遵循用户指令。

**特点**：
- 输入通常为自然语言指令
- 输出为满足指令需求的响应
- 显著提升模型的可控性和易用性

**代表方法**：
- InstructGPT（使用RLHF改进）
- Instruct-T5
- Llama-2 Chat

## 对齐（Alignment）

**目标**：使模型的行为与人类价值观和偏好对齐。

**关键方法**：
- **人类反馈强化学习（RLHF）**
  - 收集人类反馈
  - 训练奖励模型
  - 使用RL优化策略
  
- **直接偏好优化（DPO）**
  - 无需奖励模型
  - 直接学习偏好

## 知识蒸馏（Knowledge Distillation）

**原理**：用小模型学习大模型的知识。

**过程**：
1. 使用大模型（教师）处理样本
2. 小模型（学生）同时学习原始标签和教师输出
3. 通过KL散度匹配输出分布

**优势**：
- 显著压缩模型
- 保持性能
- 降低推理成本

## 量化（Quantization）

**定义**：将模型权重从高精度（如float32）转换为低精度（int8、int4、fp8等）。

**类型**：
- **静态量化**：预定义量化方案
- **动态量化**：运行时确定量化参数

**效果**：
- 模型体积减小4-8倍
- 推理速度提升
- 内存占用大幅降低
- 精度损失通常在1-3%

---

# 关键术语对比与应用

## 预训练 vs 微调

| 特性 | 预训练 | 微调 |
|------|--------|------|
| **目标** | 学习通用语言表示 | 适应特定下游任务 |
| **数据** | 大规模无标注数据 | 中等规模标注数据 |
| **时间** | 数周至数月 | 数小时至数天 |
| **计算** | 海量GPU/TPU | 中等计算资源 |
| **参数更新** | 全部参数 | 全部或部分参数 |

## 编码器 vs 解码器 vs 编码-解码

| 模型 | 结构 | 典型用途 | 代表模型 |
|------|------|---------|---------|
| **仅编码器** | 只有Transformer编码器 | 分类、序列标注、语义相似度 | BERT、RoBERTa |
| **仅解码器** | 只有Transformer解码器，自回归 | 文本生成、对话 | GPT系列 |
| **编码-解码** | 完整Transformer架构 | 机器翻译、摘要、问答 | T5、BART、mT5 |

## 自监督 vs 监督 vs 无监督

| 学习方式 | 定义 | 数据需求 | 场景 |
|---------|------|---------|------|
| **自监督** | 从数据本身产生标签 | 无标注数据 | PLM预训练 |
| **监督学习** | 使用人工标注标签 | 大量标注数据 | 下游任务微调 |
| **无监督学习** | 学习数据内在结构 | 无标注数据 | 聚类、降维 |

## 零样本、少样本与多样本学习

| 类型 | 示例数量 | 应用场景 | 性能 |
|------|---------|---------|------|
| **零样本(Zero-shot)** | 0 | 全新任务，仅有描述 | 基础但可接受 |
| **少样本(Few-shot)** | 2-10 | 标注数据稀缺 | 中等到较好 |
| **少量样本(Low-resource)** | 10-100 | 有限标注数据 | 较好 |
| **多样本(Many-shot)** | >100 | 充足标注数据 | 最优 |

---

## NLP 发展演进

### 重要里程碑

| 时间 | 模型 | 关键创新 |
|------|------|---------|
| 2018 | BERT | 双向预训练，MLM任务 |
| 2019 | GPT-2/XLNet | 大规模语言模型，排列语言模型 |
| 2020 | GPT-3 | 1750亿参数，In-context learning |
| 2020 | T5 | 统一文本到文本框架 |
| 2021 | ERNIE、NEZHA | 知识增强预训练 |
| 2022 | ChatGPT | 指令微调+RLHF，革命级产品 |
| 2023 | GPT-4、Claude、LLaMA | 性能突破 |
| 2024 | Claude 3、Grok、Gemini 2 | 多模态与推理能力提升 |

### 性能差异对比

```
参数规模 vs 性能：非线性关系
├─ 10M～100M：任务特定模型
├─ 1B～10B：通用预训练模型（BERT级别）
├─ 10B～100B：强大预训练模型（GPT-3级别）
└─ 100B+：超大模型，涌现能力显著（GPT-4、Claude级别）
```

---

## 总结对比表

### 各架构特性总结

| 特性 | FNN | CNN | RNN/LSTM | Transformer |
|------|-----|-----|----------|------------|
| **适用数据** | 表格 | 图像 | 序列 | 任意长序列 |
| **参数效率** | 低 | 中 | 中 | 高 |
| **并行能力** | 强 | 强 | 弱 | 强 |
| **长距离依赖** | 无 | 有限 | 中等 | 强 |
| **计算复杂度** | $O(n)$ | $O(n)$ | $O(n)$ | $O(n^2)$ |
| **成熟度** | 极高 | 极高 | 高 | 当前主流 |

### PLM/LLM 选择指南

| 需求 | 模型建议 | 理由 |
|------|---------|------|
| **分类任务** | BERT/RoBERTa | 双向理解优势 |
| **文本生成** | GPT/LLaMA | 自回归特性适合生成 |
| **翻译/摘要** | T5/BART | 编码-解码架构 |
| **对话系统** | ChatGPT/Claude | 指令微调+对齐优势 |
| **个性化应用** | 量化LLaMA/Mistral | 开源便宜，可本地部署 |
| **资源极限** | MobileBERT/DistilBERT | 高效模型 |
